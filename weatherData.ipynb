{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a89e5408-0315-4421-b6db-bb2a8382aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset, load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "# Log in to Hugging Face\n",
    "HUGGINGFACE_TOKEN = \"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\"  \n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "21a1c707-2a8a-4356-94bc-53cef4fe715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset_from_huggingface(repo_name, file_name):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from a Hugging Face repository.\n",
    "\n",
    "    Parameters:\n",
    "        repo_name (str): Name of the Hugging Face repository.\n",
    "        file_name (str): Name of the dataset file in the repository.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Push the dataset to Hugging Face Hub\n",
    "    repo_name = \"andreitut/merged_kth_dataset\"  # Replace with your desired repository name\n",
    "   \n",
    "    repo_url = f\"https://huggingface.co/datasets/{repo_name}/resolve/main/{file_name}\"\n",
    "    dataset = pd.read_csv(repo_url, parse_dates=[\"time\"], index_col=\"time\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "448b5431-ae36-4f39-b310-1b01636536c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incremental_weather_data(latitude, longitude, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch incremental weather data from Open-Meteo API for a specific date range.\n",
    "    \"\"\"\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": [\"temperature_2m\", \"precipitation\", \"wind_speed_10m\", \"wind_direction_10m\"],\n",
    "        \"timezone\": \"auto\",\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract hourly data\n",
    "    hourly_data = data[\"hourly\"]\n",
    "    weather_df = pd.DataFrame(hourly_data)\n",
    "\n",
    "    # Ensure 'time' exists and is in proper datetime format\n",
    "    if \"time\" in weather_df.columns:\n",
    "        weather_df['time'] = pd.to_datetime(weather_df['time'], errors='coerce')\n",
    "        if weather_df['time'].isnull().any():\n",
    "            raise ValueError(\"Invalid time format detected in API response.\")\n",
    "    else:\n",
    "        raise KeyError(\"'time' column missing in the API response.\")\n",
    "\n",
    "    # Set 'time' as the index\n",
    "    weather_df.set_index('time', inplace=True)\n",
    "\n",
    "    # Debug outputs\n",
    "    print(\"Index type:\", weather_df.index.dtype)  # Confirm datetime\n",
    "    print(\"Columns:\", weather_df.columns)  # Verify other columns\n",
    "    return weather_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c37a9c64-94af-4077-97da-1e76a16a3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_push_dataset(repo_name, latitude, longitude):\n",
    "    \"\"\"\n",
    "    Updates the weather dataset and pushes it to Hugging Face.\n",
    "    \"\"\"\n",
    "    # Step 1: Load the existing dataset\n",
    "    print(\"Loading existing dataset from Hugging Face...\")\n",
    "    existing_data = load_existing_dataset(repo_name)\n",
    "\n",
    "    if not existing_data.empty:\n",
    "        last_date = existing_data.index.max().strftime('%Y-%m-%d')  # Access index\n",
    "    else:\n",
    "        last_date = \"2024-01-01\"  # Default start date if dataset is empty\n",
    "\n",
    "    # Step 2: Fetch new data if needed\n",
    "    start_date = pd.Timestamp(last_date) + pd.Timedelta(days=1)\n",
    "    end_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    if start_date <= pd.Timestamp(end_date):\n",
    "        print(f\"Fetching weather data from {start_date} to {end_date}...\")\n",
    "        new_data = get_incremental_weather_data(latitude, longitude, start_date.strftime('%Y-%m-%d'), end_date)\n",
    "\n",
    "        print(\"Fetched data columns:\", new_data.columns)\n",
    "\n",
    "        # Combine existing data with the new data\n",
    "        updated_data = pd.concat([existing_data, new_data])\n",
    "\n",
    "        # Handle duplicates and sort by index (time)\n",
    "        updated_data = updated_data[~updated_data.index.duplicated(keep='last')].sort_index()\n",
    "    else:\n",
    "        print(\"No new data to fetch. Dataset is already up-to-date.\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Push updated dataset to Hugging Face\n",
    "    print(\"Uploading updated dataset to Hugging Face...\")\n",
    "    hf_dataset = Dataset.from_pandas(updated_data)\n",
    "    hf_dataset.push_to_hub(repo_name)\n",
    "    print(f\"Dataset successfully uploaded to Hugging Face Hub: {repo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a56aa82-6cd2-48a1-a124-d6a910ae04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the dataset from Hugging Face\n",
    "def load_existing_dataset(repo_name):\n",
    "    \"\"\"\n",
    "    Loads the existing dataset from Hugging Face, if available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset from Hugging Face\n",
    "        hf_dataset = Dataset.load_from_hub(repo_name)\n",
    "        df = hf_dataset.to_pandas()\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        return df\n",
    "    except:\n",
    "        print(\"No existing dataset found. Starting fresh.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0878035-303c-4da0-b6bd-ed9a239e37b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset from Hugging Face...\n",
      "No existing dataset found. Starting fresh.\n",
      "Fetching weather data from 2024-01-02 00:00:00 to 2024-12-28...\n",
      "Index type: datetime64[ns]\n",
      "Columns: Index(['temperature_2m', 'precipitation', 'wind_speed_10m',\n",
      "       'wind_direction_10m'],\n",
      "      dtype='object')\n",
      "Fetched data columns: Index(['temperature_2m', 'precipitation', 'wind_speed_10m',\n",
      "       'wind_direction_10m'],\n",
      "      dtype='object')\n",
      "Uploading updated dataset to Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "ploading the dataset shards: 100%|███████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to Hugging Face Hub: andreitut/weatherDatasetProject\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "repo_name = \"andreitut/weatherDatasetProject\"  # Replace with your repository name\n",
    "latitude, longitude = 59.3293, 18.0686  # Stockholm coordinates\n",
    "\n",
    "\n",
    "# Run the update process\n",
    "update_and_push_dataset(repo_name, latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c932348-6003-4888-b87c-77543efd5b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861d518-82a6-47d0-bae5-57c9e57dbd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e38030-4a12-41e3-87ac-51a99baee894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56496c-9c05-4022-b67e-968d1e600f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403b54e-03ab-4881-9e2d-f2cbafb82994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
